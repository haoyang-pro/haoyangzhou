---
title: 大语言模型中的Token计算与费用控制
date: 2025-04-09 21:20:06
categories: [生成式AI]
tags: [LLM, Token计算, 成本优化]
mathjax: true

---

### 什么是Token？

Token 是文本的基本单位，可以是单词、子词、标点符号或符号。每当您向 API 发送请求时，文本会被分解成 tokens，以便模型理解并生成响应。

对中文而言，1 个汉字通常算作 1 个 Token；对英文而言，分为长单词和短单词。其中，短单词例如“a”或"the"，占 1 个 Token。长单词，会被拆分成多个 Token。

例如，“hello world”，占用 2 个 Token；“chatgpt api“，占 3 个 Token，平均 4-5 个英文字符算作一个 Token。

通常在 api 计费时会谈到一个词，叫做百万 Token，也就是 1M Tokens = 1,000,000 Tokens。因此：

如果是中文，每百万 Token 大约相当于 70-100 万个汉字。
如果是英文，每百万 Token 大约相当于 50-75 万个单词。

### 多轮对话中的Token
在多轮对话的上下文，一般包含如下内容：1. 用户的历史输入；2.模型的历史回复；3.系统提示（如设定聊天机器人的角色或目标等）。上下文越长，Token 数量就越多。

#### 上下文窗口的限制
每个大语言模型都有“上下文窗口大小”（Context Window）的限制。常见的上下文窗口例如：4k Tokens（如 OpenAI GPT-3.5）；8k、32k Tokens（如 GPT-4 或其他支持长上下文的模型）。此外，超过上下文窗口大小的内容会被截断或需要人工实现“摘要化”。

#### 多轮对话的上下文Token累积
假设每轮对话的输入和输出分别是 50 Token 和 100 Token：

第 1 轮：50（输入） + 100（输出） \= 150 Token。
第 2 轮：50（新输入） + 100（新输出） + 150（历史上下文） \= 300 Token。
第 3 轮：50（新输入） + 100（新输出） + 300（历史上下文） \= 450 Token。

以此类推，Token 数随着对话轮数增加逐渐累积。

#### 应该如何优化多轮对话应用的Token使用
优化 Token 使用数量 在使用大语言模型开发应用时非常重要，首先是成本控制，毕竟 API 的计费都是按照 Token 总量来累积计算的，优化 Token 使用方式，可以显著降低调用成本；其次，处理时间与 Token 数量也成正比，需要处理的 Token 数越多，响应时间越长，尤其是输出的时候也需要大量 Token 时；另外，上下文窗口限制，决定了优化可以减少历史上下文的丢失，提高连贯性和上下文一致性，以及不至于超出窗口限制带来更差的结果。最后，啰嗦的表达也有可能会降低模型回答的质量。

优化 Token 使用的具体策略有许多。以下几个比较常规：

- 精简输入内容，目标是减少输入的冗余信息，让模型更精准地理解用户意图。
- 使用简洁的问题或命令，去除多余的背景信息，确保用户输入内容的简洁明了，且只包含核心问题或需求，避免包含无关或重复信息。
- 减少格式化内容，例如无用的空格、换行或特殊符号等。
- 限制输出长度，目标是控制模型生成的内容长度，避免长篇大论。
- 设置 max_tokens 参数，避免生成过长回答。
- 在提示中明确要求简洁回答。
- 管理上下文历史，目标是减少对历史对话的占用，提升效率。
- 使用滑动窗口机制，仅保留最近几轮重要的对话内容。例如在多轮对话中，仅保留最近 3-5 轮的关键上下文。
- 对较早的上下文生成摘要，总结成一段简短的文字代替，减少冗余。
- 自动检测和优化输入，目标是帮助用户清晰表达需求，减少误解和多余信息。
- 开发预处理模块，去掉用户输入中的多余字符或句子。
- 提供输入模板，指导用户提交简洁内容。

### 多轮对话的Token成本
#### Token预估
假设预估平均每轮对话，输入 Token：50-100，输出 Token：150-250，上下文 Token 累积：300-400（按历史对话内容）。
不加控制时，Token 使用量会迅速增长，但通过限制上下文和输出长度可以显著减少消耗。
#### 限制上下文后的成本预估
假设使用 OpenAI 的 GPT-3.5 Turbo 模型，价格参考，输入：\$0.0015 / 千 Token；输出：\$0.002 / 千 Token。
##### 情景1：不限制上下文
假设对话至第 10 轮，累计上下文约为 3000 Tokens：
输入：3000 × \$0.0015 = $0.0045。

输出：250 × \$0.002 = $0.0005。

单轮总成本： $0.005。
##### 情景2：限制上下文为最近3轮
假设每轮上下文约 900 Tokens：
输入：900 × \$0.0015 = $0.00135。

输出：250 × \$0.002 = $0.0005。

单轮总成本： $0.00185。

**结论**：限制上下文后，单轮对话成本减少约 60%。
